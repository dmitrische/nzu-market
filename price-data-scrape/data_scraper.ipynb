{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a8914f",
   "metadata": {},
   "source": [
    "# NZU price data scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b35ad",
   "metadata": {},
   "source": [
    "To scrape the data using a Python package called Beautiful Soup, we need to first use Python's `request` module to get the entire HTML source of a given website. We are interested in two URLs:\n",
    "```\n",
    "https://www.carbonnews.co.nz/tag.asp?tag=Jarden+NZ+Market+Report\n",
    "https://www.carbonnews.co.nz/tagarchive.asp?tag=Jarden+NZ+Market+Report\n",
    "```\n",
    "So let us get and parse them separately, and store them as two beautiful soups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe203c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url=\"https://www.carbonnews.co.nz/\"\n",
    "\n",
    "page = requests.get(url+\"tag.asp?tag=Jarden+NZ+Market+Report\")\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "page = requests.get(url+\"tagarchive.asp?tag=Jarden+NZ+Market+Report\")\n",
    "soup2 = BeautifulSoup(page.content, \"html.parser\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c693a4",
   "metadata": {},
   "source": [
    "Using our knowledge of the underlying HTML structure, we can pick out the headline and summary elements for each story, in both soups, and store the extracted elements in lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d3284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the h1, h2, and h3 headlines in first soup\n",
    "helements = soup.find_all([\"h1\",\"h2\",\"h3\"], class_=\"Headline\")\n",
    "# Find the h3 headlines in second soup\n",
    "helements+= soup2.find_all(\"h3\", class_=\"Headline\")\n",
    "\n",
    "# Find all the accompanying summaries\n",
    "pelements = soup.find_all(\"p\", class_=None ) # the h1 headline\n",
    "pelements+= soup.find_all(\"p\", class_=[\"StoryIntro\",\"StoryIntro_small\"]) # h2 and h3 headlines\n",
    "pelements+= soup2.find_all(\"p\", class_=\"StoryIntro_small\") # h3 headlines from the second soup\n",
    "\n",
    "# Find all h4 headlines in the second soup\n",
    "helements_arch = soup2.find_all([\"h4\"], class_=\"Headline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49bfb9",
   "metadata": {},
   "source": [
    "Note that `helements` and `pelements` should be commensurate, while `helements_archive` should be much longer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da90a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39\n",
      "2764\n",
      "<h4 class=\"Headline\" xstyle=\"font-weight:normal;\"><img alt=\"\" height=\"8\" src=\"images/arrow.gif\" width=\"8\"> 14 Jun 23  <a href=\"story.asp?storyID=27947\">ETS auction fails to clear</a></img></h4>\n"
     ]
    }
   ],
   "source": [
    "print(len(helements), len(pelements))\n",
    "print(len(helements_arch))\n",
    "print(helements_arch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d79fa",
   "metadata": {},
   "source": [
    "Let us now extract just the three relevant bits of text for each and every story: the headline string, the date string, and the href string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8f631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = []; datestrings = []; hrefs = []\n",
    "\n",
    "# First loop over more recent headlines with summaries\n",
    "for i in range(len(helements)):\n",
    "    body = helements[i].find(\"a\")\n",
    "    headlines.append(body.text.strip())\n",
    "    hrefs.append(body.get(\"href\"))\n",
    "    # The date string is the first text before the ' - ' in the summary.\n",
    "    datestrings.append(pelements[i].text.strip().split(' - ')[0])\n",
    "        \n",
    "# Then loop over older archived headlines without summaries\n",
    "for h in helements_arch:\n",
    "    body = h.find(\"a\")\n",
    "    headline = body.text.strip()\n",
    "    headlines.append(headline)\n",
    "    hrefs.append(body.get(\"href\"))\n",
    "    # The date string is the text that's not part of the headline \n",
    "    datestrings.append(h.text.strip().replace(headline,''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faecda30",
   "metadata": {},
   "source": [
    "Check that the three lists are of the same length, and then print the first few and the last few entries in each list, just for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5bf633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2803 2803 2803 \n",
      "\n",
      "Thursday  |  MARKET LATEST: NZUs $60.75  |  story.asp?storyID=28419\n",
      "Wednesday  |  MARKET LATEST: NZUs $60.25  |  story.asp?storyID=28409\n",
      "Tuesday  |  MARKET LATEST: NZUs $59.90  |  story.asp?storyID=28396\n",
      "Monday  |  MARKET LATEST: NZUs $58.00  |  story.asp?storyID=28382\n",
      "4 Aug 23  |  MARKET LATEST: NZUs $58.00  |  story.asp?storyID=28371\n",
      "3 Aug 23  |  MARKET LATEST: NZUs $57.00  |  story.asp?storyID=28360\n",
      "2 Aug 23  |  MARKET LATEST: NZUs $57.50  |  story.asp?storyID=28345\n",
      "1 Aug 23  |  MARKET LATEST: NZUs $59.75  |  story.asp?storyID=28334\n",
      "\n",
      "13 Jun 08    |  Current carbon credits available  |  story.asp?storyID=1194\n",
      "13 Jun 08    |  Latest strip of CERs   2008  2012 vintage  indicative mid prices  |  story.asp?storyID=1193\n",
      "17 Jun 08    |  Current Carbon Credits Available  |  story.asp?storyID=1219\n",
      "17 Jun 08    |  Latest Strip of CERs   2008  2012 Vintage  Indicative Mid Prices  |  story.asp?storyID=1218\n",
      "17 Jun 08    |  Oil pushes carbon higher  |  story.asp?storyID=1217\n",
      "18 Jun 08    |  Current Carbon Credits Available  |  story.asp?storyID=1226\n",
      "18 Jun 08    |  Latest Strip of CERs   2008  2012 Vintage  Indicative Mid-Prices  |  story.asp?storyID=1225\n",
      "18 Jun 08    |  Carbon makes small gains  |  story.asp?storyID=1224\n"
     ]
    }
   ],
   "source": [
    "print(len(headlines), len(datestrings), len(hrefs),'\\n')\n",
    "\n",
    "for i in range(8):\n",
    "    print(datestrings[i],' | ',headlines[i],' | ',hrefs[i])\n",
    "\n",
    "print()\n",
    "\n",
    "for i in range(1,9):\n",
    "    print(datestrings[-i],' | ',headlines[-i],' | ',hrefs[-i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bc4c6d",
   "metadata": {},
   "source": [
    "Convert date-strings to date objects using the `datetime` module. For later reuse, I implemented the conversion as a function called `strings2dates` inside the `functions.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22008737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraping_functions import strings2dates\n",
    "dates = strings2dates(datestrings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca502103",
   "metadata": {},
   "source": [
    "We know that the price of NZUs is first reported on 14 May 2010, so we can discard older stories from our lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b2ce2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "while dates[-1] < datetime(2010,5,14).date():\n",
    "    dates.pop()\n",
    "    datestrings.pop()\n",
    "    headlines.pop()\n",
    "    hrefs.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea3b51f",
   "metadata": {},
   "source": [
    "Print the date, headline, and hrefs for oldest few stories still  remaining in the list, just for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf3e011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-05-14  |  NZUs have quiet week trading in the $17s  |  story.asp?storyID=4529\n",
      "2010-05-21  |  Carbon trading in the high teens  |  story.asp?storyID=4540\n",
      "2010-05-28  |  Carbon price consolidates  |  story.asp?storyID=4551\n",
      "2010-06-11  |  Carbon moves up to mid $17s  |  story.asp?storyID=4588\n",
      "2010-06-25  |  Demand builds in local market  |  story.asp?storyID=4623\n",
      "2010-07-02  |  Carbon price firm, but supply is short  |  story.asp?storyID=4643\n",
      "2010-07-09  |  Sellers few and far between  |  story.asp?storyID=4658\n",
      "2010-07-16  |  Market firm but drop on the way  |  story.asp?storyID=4675\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,9):\n",
    "    print(dates[-i],' | ',headlines[-i],' | ',hrefs[-i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1368130e",
   "metadata": {},
   "source": [
    "Now print all the remaining stories' date, headline, and URL to a CSV file called `headline_scrape.csv`, with the stories ordered from oldest to latest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c36f019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "url = 'https://www.carbonnews.co.nz/'\n",
    "with open('headline_scrape.csv', 'w', newline='') as file:    \n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['date','headline','url'])\n",
    "    for i in range(1,1+len(dates)):\n",
    "        writer.writerow([dates[-i],headlines[-i],url+hrefs[-i]])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "204b9044",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3482318710.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [10], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def parse_headline(headling)\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "def parse_headline(headling)\n",
    "\"\"\"\n",
    "Function taking a list of strings and returning a list of dollar-valued prices\n",
    "extracted from each string.\n",
    "\n",
    "INPUTS:\n",
    "------\n",
    "\n",
    "headline: a string with headline text\n",
    "\n",
    "OUTPUTS:\n",
    "-------\n",
    "\n",
    "price: a real-valued price extracted from headline\n",
    "\"\"\"\n",
    "\n",
    "words = headline.split()\n",
    "keywords = ['NZU', 'settles']\n",
    "\n",
    "price = None\n",
    "if('MARKET LATEST' in headline):\n",
    "    pricestring = words[-1].strip('$')\n",
    "    try:\n",
    "        price = float(pricestring)\n",
    "    except ValueError:\n",
    "        print(\"WARNING: '{}' is not a float!\".format(pricestring))\n",
    "elif():\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5fc8e3",
   "metadata": {},
   "source": [
    "Furthermore, the date is not actually included in the headlines, but it nonetheless can still be scraped from just the two main web pages, though the date formatting there makes it more awkward to parse than when scraping from individual story pages. \n",
    "\n",
    "So, for demonstration purposes, let us take a multi-prong approach with some redundancies, and we will two approaches using different toolsets:\n",
    "- [Bash](https://www.gnu.org/software/bash/) and Linux command-line tools to scrape just the spot price from the headlines; and\n",
    "- Python package called [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) to get the spot prices from the headings, and then also look for the headline price in the story's first paragraph. \n",
    "\n",
    "\n",
    "## Command-line approach\n",
    "\n",
    "We are interested in two URLs:\n",
    "```\n",
    "https://www.carbonnews.co.nz/tag.asp?tag=Jarden+NZ+Market+Report\n",
    "https://www.carbonnews.co.nz/tagarchive.asp?tag=Jarden+NZ+Market+Report\n",
    "```\n",
    "\n",
    "First download the HTML source using `wget`, then use `grep`, `sed`, and `awk` to get the values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f251447",
   "metadata": {},
   "source": [
    "## Beautiful Soup approach\n",
    "\n",
    "All CarbonNews stories can be accessed via public URLs such as \n",
    "```\n",
    "https://www.carbonnews.co.nz/story.asp?storyID=28274\n",
    "```\n",
    "for the story pictured above. Given such a URL, we can obtain the page HTML code and scrape the required information from that. Conveniently, the HTML source follows a fairly simple structure, and the entire text snippet can be extracted by   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97e305d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(headline):\n",
    "    \n",
    "    if('MARKET LATEST:' in headline or \n",
    "       ('NZU' in headline and headline.count('$') == 1)):\n",
    "        result = True\n",
    "    else:\n",
    "        result = False\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3180166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stories(hrefs):\n",
    "    \n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    from datetime import datetime\n",
    "    \n",
    "    data = []\n",
    "    url_home=\"https://www.carbonnews.co.nz/\"\n",
    "    \n",
    "    for href in hrefs:\n",
    "        \n",
    "        url = url_home+href\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        \n",
    "        # Process the heading, which should be the first 'h1' element in soup\n",
    "        h1elements = soup.find_all(\"h1\", class_=\"story\")\n",
    "        heading = h1elements[0].text.strip()\n",
    "        prices = [word.strip('$.') for word in heading.split() if word[0] == '$']\n",
    "        if len(prices) == 1:\n",
    "            price = prices[0]\n",
    "        else:\n",
    "            price = 'NaN'\n",
    "            if len(price) > 1:\n",
    "                print('WARNING: Heading contains multiple dollar values.')\n",
    "            else:\n",
    "                print('WARNING: Heading contains no dollar values.')        \n",
    "            \n",
    "        \n",
    "        # Process text snippet, which should be the only div element of class StoryFirstPara in soup\n",
    "        para = soup.find(\"div\", class_=\"StoryFirstPara\").text.strip()\n",
    "        prices = [word.strip('$.') for word in para.split() if word[0] == '$']\n",
    "        if price not in prices:\n",
    "            print('WARNING: Heading price not in story text snippet.')\n",
    "        \n",
    "        # Process the story date\n",
    "        h4elements = soup.find_all(\"h4\")\n",
    "        date = h4elements[0].text.strip()        \n",
    "        if date.split()[0] == 'Today':\n",
    "            date = datetime.today().strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            date = ' '.join(date.split()[1:-1]) # strip weekday and time\n",
    "            date = datetime.strptime(date,'%d %b %y').strftime('%Y-%m-%d')\n",
    "        \n",
    "        data.append({'date':date, 'price':price, 'url':url})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70a265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
