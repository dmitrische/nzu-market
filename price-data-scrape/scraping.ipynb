{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a8914f",
   "metadata": {},
   "source": [
    "# NZU price data scraping\n",
    "\n",
    "[CarbonNews](https://www.carbonnews.co.nz/) is described as \"New Zealand’s only daily news service covering the carbon markets, climate change, sustainable business and the growth of the low-carbon economy\". It is a private business, so to read the published stories in full one must pay a subscription fee. However, some useful information can be gleaned from headlines and short text summaries visible to non-subscribers. \n",
    "\n",
    "The [Jarden NZ Market Report](https://www.carbonnews.co.nz/tag.asp?tag=Jarden+NZ+Market+Report) section lists stories with updates on the price of NZUs traded on [CommTrade](https://www.commtrade.co.nz/). Each story (such as [this](https://www.carbonnews.co.nz/story.asp?storyID=28274) one) reports on the latest \"fixing\", i.e. spot price, as well as the opening bid and offer prices. Since early 2016, the spot price has been quoted in every story's headline, while the opening bid and offer prices are given in the accompanying summary. For older archived stories, however, the spot price is not always featured in the headline but only in the summary, and the formatting of both the headline and the summary is less consistent.\n",
    "\n",
    "![CarbonNews story](./CarbonNewsStory.png)\n",
    "\n",
    "Note that the price history plotted in the image accompanying each story covers only the last six months, whereas the [Jarden NZ Market Report](https://www.carbonnews.co.nz/tag.asp?tag=Jarden+NZ+Market+Report) section and its [archive](https://www.carbonnews.co.nz/tagarchive.asp?tag=Jarden+NZ+Market+Report) go back years. Hence, we would like to scrape all the available price values, with the corresponding date and source story URL, and save this data to a Comma Separated Values (CSV) file in the follow format:\n",
    "```\n",
    "\"date\",\"price\",\"source\"\n",
    "24-07-2023,47.25,'https://www.carbonnews.co.nz/story.asp?storyID=28263'\n",
    "25-07-2023,50.00,'https://www.carbonnews.co.nz/story.asp?storyID=28274'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b50e9",
   "metadata": {},
   "source": [
    "In general, data scraping involves parsing the HTML source code of a web page of interest, and in the present case we have two to contend with: [Jarden NZ Market Report](https://www.carbonnews.co.nz/tag.asp?tag=Jarden+NZ+Market+Report) and its [archive](https://www.carbonnews.co.nz/tagarchive.asp?tag=Jarden+NZ+Market+Report). These two web pages contain all the information required to produce the desired CSV file, but only as far back as February 2016. For hundreds of older stories, to get the price value we will need to find and parse the web page for each story individually. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b8174",
   "metadata": {},
   "source": [
    "## Inspection using browser\n",
    "From simply looking at the [Jarden NZ Market Report](https://www.carbonnews.co.nz/tag.asp?tag=Jarden+NZ+Market+Report) web page in a browser we see a listing of stories, each with a clickable headline, a brief text summary, and a graph. Most (if not all) of the headlines state the spot price of NZUs, and each accompanying summary begins with the date when the story was published. Somewhat inconveniently, the date formatting is variable: showing just \"Today\" or the appropriate weekday for stories that are less than a week old, and the date in full (e.g. \"25 Jul 23\") only for older stories.  \n",
    "\n",
    "![](./Report_h1h2.png)\n",
    "![](./Report_h2h3.png)\n",
    "\n",
    "Inspecting the HTML source reveals that the central listing of stories is associated with a `<div>` element of class `\"StoryList\"`. Inside this element, the latest headline in the listing is associated with the element tagged by `<h1>`, the following six headlines are each tagged by `<h2>`, and the remaining ones by `<h3>`; and all these elements have the same class name `\"Headline\"` attributed to them. The actual headline text is nested inside an `<a>` sub-element with an `href` attribute (defining a hyperlink to the full story). Furthermore, each and every headline element is followed by an accompanying `<p>` element containing the story's brief summary.       \n",
    "\n",
    "Inspection of [Jarden NZ Market Report Archive](https://www.carbonnews.co.nz/tagarchive.asp?tag=Jarden+NZ+Market+Report) shows continuation of the same general pattern: first twenty stories in the archive are associated with `<h3>` elements (containing the headline) and accompanying `<p>` elements (containing the summary); while all older stories are tagged by `<h4>` (without any accompanying `<p>` elements). For these older archived stories, the full date is embedded in the corresponding `<h4>` element but outside the internal `<a>` sub-element.\n",
    "\n",
    "![](./Archive_h3.png)\n",
    "![](./Archive_h3h4.png)\n",
    "\n",
    "Now, having gleaned the underlying HTML structure, we can proceed with the actual data scraping. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b35ad",
   "metadata": {},
   "source": [
    "## Scraping with Beautiful Soup\n",
    "\n",
    "To scrape the data using a Python package called Beautiful Soup, we need to first use Python's `request` module to get the entire HTML source of a given website. We are interested in two URLs:\n",
    "```\n",
    "https://www.carbonnews.co.nz/tag.asp?tag=Jarden+NZ+Market+Report\n",
    "https://www.carbonnews.co.nz/tagarchive.asp?tag=Jarden+NZ+Market+Report\n",
    "```\n",
    "So let us get and parse them separately, and store them as two beautiful soups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe203c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url=\"https://www.carbonnews.co.nz/\"\n",
    "\n",
    "page = requests.get(url+\"tag.asp?tag=Jarden+NZ+Market+Report\")\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "page = requests.get(url+\"tagarchive.asp?tag=Jarden+NZ+Market+Report\")\n",
    "soup2 = BeautifulSoup(page.content, \"html.parser\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c693a4",
   "metadata": {},
   "source": [
    "Using our knowledge of the underlying HTML structure, we can pick out the headline and summary elements for each story in the soups, and store the extracted elements in lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d3284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the h1, h2, and h3 headlines in first soup\n",
    "helements = soup.find_all([\"h1\",\"h2\",\"h3\"], class_=\"Headline\")\n",
    "# Find the h3 headlines in second soup\n",
    "helements+= soup2.find_all(\"h3\", class_=\"Headline\")\n",
    "\n",
    "# Find all the accompanying summaries\n",
    "pelements = soup.find_all(\"p\", class_=None ) # the h1 headline\n",
    "pelements+= soup.find_all(\"p\", class_=[\"StoryIntro\",\"StoryIntro_small\"]) # h2 and h3 headlines\n",
    "pelements+= soup2.find_all(\"p\", class_=\"StoryIntro_small\") # h3 headlines from the second soup\n",
    "\n",
    "# Find all h4 headlines in the second soup\n",
    "helements_arch = soup2.find_all([\"h4\"], class_=\"Headline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49bfb9",
   "metadata": {},
   "source": [
    "Note that `helements` and `pelements` should be commensurate, while `helements_archive` should be much longer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da90a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39\n",
      "2760\n",
      "<h4 class=\"Headline\" xstyle=\"font-weight:normal;\"><img alt=\"\" height=\"8\" src=\"images/arrow.gif\" width=\"8\"> 9 Jun 23  <a href=\"story.asp?storyID=27911\">Failure is not an auction</a></img></h4>\n"
     ]
    }
   ],
   "source": [
    "print(len(helements), len(pelements))\n",
    "print(len(helements_arch))\n",
    "print(helements_arch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d79fa",
   "metadata": {},
   "source": [
    "Let us now extract just the three relevant bits of text for each and every story: the headline string, the date string, and the href string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb8f631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = []; datestrings = []; hrefs = []\n",
    "\n",
    "# First loop over more recent headlines with summaries\n",
    "for i in range(len(helements)):\n",
    "    body = helements[i].find(\"a\")\n",
    "    headlines.append(body.text.strip())\n",
    "    hrefs.append(body.get(\"href\"))\n",
    "    # The date string is the first text before the ' - ' in the summary.\n",
    "    datestrings.append(pelements[i].text.strip().split(' - ')[0])\n",
    "        \n",
    "# Then loop over older archived headlines without summaries\n",
    "for h in helements_arch:\n",
    "    body = h.find(\"a\")\n",
    "    headline = body.text.strip()\n",
    "    headlines.append(headline)\n",
    "    hrefs.append(body.get(\"href\"))\n",
    "    # The date string is the text that's not part of the headline \n",
    "    datestrings.append(h.text.strip().replace(headline,''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faecda30",
   "metadata": {},
   "source": [
    "Check that the three lists are of the same length, and then print the first few and the last few entries in each list, just for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5bf633c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2799 2799 2799 \n",
      "\n",
      "Friday  |  MARKET LATEST: NZUs $58.00  |  story.asp?storyID=28371\n",
      "Thursday  |  MARKET LATEST: NZUs $57.00  |  story.asp?storyID=28360\n",
      "Wednesday  |  MARKET LATEST: NZUs $57.50  |  story.asp?storyID=28345\n",
      "Tuesday  |  MARKET LATEST: NZUs $59.75  |  story.asp?storyID=28334\n",
      "Monday  |  MARKET LATEST: NZUs $60.00  |  story.asp?storyID=28323\n",
      "28 Jul 23  |  MARKET LATEST: NZUs $61.50  |  story.asp?storyID=28310\n",
      "27 Jul 23  |  MARKET LATEST: NZUs $65.25  |  story.asp?storyID=28298\n",
      "26 Jul 23  |  MARKET LATEST: NZUs $65.00  |  story.asp?storyID=28285\n",
      "\n",
      "13 Jun 08    |  Current carbon credits available  |  story.asp?storyID=1194\n",
      "13 Jun 08    |  Latest strip of CERs   2008  2012 vintage  indicative mid prices  |  story.asp?storyID=1193\n",
      "17 Jun 08    |  Current Carbon Credits Available  |  story.asp?storyID=1219\n",
      "17 Jun 08    |  Latest Strip of CERs   2008  2012 Vintage  Indicative Mid Prices  |  story.asp?storyID=1218\n",
      "17 Jun 08    |  Oil pushes carbon higher  |  story.asp?storyID=1217\n",
      "18 Jun 08    |  Current Carbon Credits Available  |  story.asp?storyID=1226\n",
      "18 Jun 08    |  Latest Strip of CERs   2008  2012 Vintage  Indicative Mid-Prices  |  story.asp?storyID=1225\n",
      "18 Jun 08    |  Carbon makes small gains  |  story.asp?storyID=1224\n"
     ]
    }
   ],
   "source": [
    "print(len(headlines), len(datestrings), len(hrefs),'\\n')\n",
    "\n",
    "for i in range(8):\n",
    "    print(datestrings[i],' | ',headlines[i],' | ',hrefs[i])\n",
    "\n",
    "print()\n",
    "\n",
    "for i in range(1,9):\n",
    "    print(datestrings[-i],' | ',headlines[-i],' | ',hrefs[-i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22008737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a5fc8e3",
   "metadata": {},
   "source": [
    "Furthermore, the date is not actually included in the headlines, but it nonetheless can still be scraped from just the two main web pages, though the date formatting there makes it more awkward to parse than when scraping from individual story pages. \n",
    "\n",
    "So, for demonstration purposes, let us take a multi-prong approach with some redundancies, and we will two approaches using different toolsets:\n",
    "- [Bash](https://www.gnu.org/software/bash/) and Linux command-line tools to scrape just the spot price from the headlines; and\n",
    "- Python package called [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) to get the spot prices from the headings, and then also look for the headline price in the story's first paragraph. \n",
    "\n",
    "\n",
    "## Command-line approach\n",
    "\n",
    "We are interested in two URLs:\n",
    "```\n",
    "https://www.carbonnews.co.nz/tag.asp?tag=Jarden+NZ+Market+Report\n",
    "https://www.carbonnews.co.nz/tagarchive.asp?tag=Jarden+NZ+Market+Report\n",
    "```\n",
    "\n",
    "First download the HTML source using `wget`, then use `grep`, `sed`, and `awk` to get the values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f251447",
   "metadata": {},
   "source": [
    "## Beautiful Soup approach\n",
    "\n",
    "All CarbonNews stories can be accessed via public URLs such as \n",
    "```\n",
    "https://www.carbonnews.co.nz/story.asp?storyID=28274\n",
    "```\n",
    "for the story pictured above. Given such a URL, we can obtain the page HTML code and scrape the required information from that. Conveniently, the HTML source follows a fairly simple structure, and the entire text snippet can be extracted by   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97e305d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(headline):\n",
    "    \n",
    "    if('MARKET LATEST:' in headline or \n",
    "       ('NZU' in headline and headline.count('$') == 1)):\n",
    "        result = True\n",
    "    else:\n",
    "        result = False\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3180166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stories(hrefs):\n",
    "    \n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    from datetime import datetime\n",
    "    \n",
    "    data = []\n",
    "    url_home=\"https://www.carbonnews.co.nz/\"\n",
    "    \n",
    "    for href in hrefs:\n",
    "        \n",
    "        url = url_home+href\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        \n",
    "        # Process the heading, which should be the first 'h1' element in soup\n",
    "        h1elements = soup.find_all(\"h1\", class_=\"story\")\n",
    "        heading = h1elements[0].text.strip()\n",
    "        prices = [word.strip('$.') for word in heading.split() if word[0] == '$']\n",
    "        if len(prices) == 1:\n",
    "            price = prices[0]\n",
    "        else:\n",
    "            price = 'NaN'\n",
    "            if len(price) > 1:\n",
    "                print('WARNING: Heading contains multiple dollar values.')\n",
    "            else:\n",
    "                print('WARNING: Heading contains no dollar values.')        \n",
    "            \n",
    "        \n",
    "        # Process text snippet, which should be the only div element of class StoryFirstPara in soup\n",
    "        para = soup.find(\"div\", class_=\"StoryFirstPara\").text.strip()\n",
    "        prices = [word.strip('$.') for word in para.split() if word[0] == '$']\n",
    "        if price not in prices:\n",
    "            print('WARNING: Heading price not in story text snippet.')\n",
    "        \n",
    "        # Process the story date\n",
    "        h4elements = soup.find_all(\"h4\")\n",
    "        date = h4elements[0].text.strip()        \n",
    "        if date.split()[0] == 'Today':\n",
    "            date = datetime.today().strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            date = ' '.join(date.split()[1:-1]) # strip weekday and time\n",
    "            date = datetime.strptime(date,'%d %b %y').strftime('%Y-%m-%d')\n",
    "        \n",
    "        data.append({'date':date, 'price':price, 'url':url})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70a265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
